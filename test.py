#!/usr/bin/env python3
"""
Magma Video Benchmark Testing Script - ä¸¥æ ¼æŒ‰ç…§åŸå§‹promptæ ¼å¼
"""

import os
import json
import re
from collections import defaultdict
import torch
from torchvision.transforms.functional import to_pil_image
from decord import VideoReader, cpu
import numpy as np
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor
from datasets import load_dataset
import pandas as pd

class VideoBenchmarkTester:
    def __init__(self, model_name="microsoft/Magma-8B"):
        self.dtype = torch.bfloat16
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_name}")
        print(f"ğŸ–¥ï¸  å¯ç”¨GPU: {torch.cuda.device_count()}")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            trust_remote_code=True, 
            torch_dtype=self.dtype,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
            low_cpu_mem_usage=True
        )
        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        print("âœ… æ¨¡å‹å·²åˆ†å¸ƒåˆ°å¤šä¸ªGPU")
        
        self.results = []

    def load_video(self, video_path, max_frames_num=32):
        """åŠ è½½è§†é¢‘ - å®Œå…¨æŒ‰ç…§åŸå§‹ä»£ç """
        if type(video_path) == str:
            vr = VideoReader(video_path, ctx=cpu(0))
        else:
            vr = VideoReader(video_path[0], ctx=cpu(0))
        total_frame_num = len(vr)
        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)
        frame_idx = uniform_sampled_frames.tolist()
        spare_frames = vr.get_batch(frame_idx).asnumpy()
        return spare_frames

    def inference_single_sample(self, example, idx):
        """æ¨ç†å•ä¸ªæ ·ä¾‹ - ä¸¥æ ¼æŒ‰ç…§åŸå§‹Magma prompt"""
        try:
            video_path = example["question_path"]
            print(f"Processing {idx} {video_path}")
            
            # å¤„ç†è§†é¢‘
            frames = self.load_video(video_path, 32)
            frames = torch.from_numpy(frames).permute(0, 3, 1, 2)
            images = [to_pil_image(frame) for frame in frames]
            
            # æ„é€ è¾“å…¥ - å®Œå…¨æŒ‰ç…§åŸå§‹ä»£ç 
            contexts = "Answer the question in this video."
            
            # åŸå§‹Magma promptæ ¼å¼
            convs = [
                {"role": "user", "content": ''.join(["<image>\n"]*len(images)) + contexts},
            ]
            convs = [
                {"role": "system", "content": "You are agent that can see, talk and act."},            
            ] + convs            
            
            prompt = self.processor.tokenizer.apply_chat_template(
                convs,
                tokenize=False,
                add_generation_prompt=True
            )

            # åŸå§‹ä»£ç çš„æ¡ä»¶åˆ¤æ–­
            if self.model.config.mm_use_image_start_end:
                prompt = prompt.replace("<image>", "<image_start><image><image_end>")
            
            inputs = self.processor(images=images, texts=prompt, return_tensors="pt")
            inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)
            inputs['image_sizes'] = inputs['image_sizes'].unsqueeze(0)
            
            # å°†inputsç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            if 'pixel_values' in inputs:
                inputs['pixel_values'] = inputs['pixel_values'].to(self.dtype)
            
            # æ¨ç†å‚æ•° - æŒ‰ç…§åŸå§‹ä»£ç 
            gen_kwargs = {
                "max_new_tokens": 1024,
                "temperature": 0,
                "do_sample": False
            }

            self.model.generation_config.pad_token_id = self.processor.tokenizer.pad_token_id
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=gen_kwargs["max_new_tokens"],
                    temperature=gen_kwargs["temperature"],
                    do_sample=gen_kwargs["do_sample"],
                )
                output = output[:, inputs["input_ids"].shape[-1] :]
                response = self.processor.decode(output[0], skip_special_tokens=True).strip()

            return {
                "idx": idx,
                "video_path": video_path,
                "ground_truth": example.get("ground_truth", ""),
                "prediction": response,
                "question_text": example.get("question_text", "")
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return {
                "idx": idx,
                "video_path": video_path,
                "ground_truth": example.get("ground_truth", ""),
                "prediction": "",
                "question_text": example.get("question_text", "")
            }

    def run_benchmark(self):
        """è¿è¡Œmorse-500æµ‹è¯•"""
        print("ğŸ”„ åŠ è½½morse-500æ•°æ®é›†")
        dataset = load_dataset('video-reasoning/morse-500')
        
        print(f"ğŸš€ å¼€å§‹å¤„ç† {len(dataset['test'])} ä¸ªæ ·æœ¬")
        
        for i, example in tqdm(enumerate(dataset['test']), desc="Processing", total=len(dataset['test'])):
            result = self.inference_single_sample(example, i)
            self.results.append(result)
        
        # ä¿å­˜ç»“æœ
        results_df = pd.DataFrame(self.results)
        results_df.to_csv("morse_500_magma_results.csv", index=False)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° morse_500_magma_results.csv")

def main():
    # è®¾ç½®å¤šGPUç¯å¢ƒ
    print(f"ğŸ–¥ï¸  æ€»GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    
    tester = VideoBenchmarkTester("microsoft/Magma-8B")
    tester.run_benchmark()

if __name__ == "__main__":
    main()